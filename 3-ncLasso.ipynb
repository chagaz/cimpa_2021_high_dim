{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e7dc6c5-6ca6-4821-8de0-3c6b0403b8a1",
   "metadata": {},
   "source": [
    "# Feature selection in high-dimensional genetic data\n",
    "\n",
    "# Notebook 3: Network-constrained Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b44fa48-2b0e-4e6f-a48a-5865b2e4f2b5",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we implement the network-constrained Lasso, which adds a regularization terms that ensures that the regression weights vary smoothly on a provided network. When the truly non-zero features are indeed connected on this network, the constraint helps identifying them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6585a4d6-d64d-4ef0-be3a-7c2693ac47dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline \n",
    "# imports matplotlib as plt and numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9173f42-b998-496d-b824-4cfd499084c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', **{'size': 14}) # font size for text on plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d2f309-b3bb-48b2-b934-b42b0f595876",
   "metadata": {},
   "source": [
    "## Conversion of the ncLasso problem into a lasso problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b402aefe-7ce5-42cc-bdc9-36c0feee9743",
   "metadata": {},
   "source": [
    "The ncLasso problem is formulated as follows:\n",
    "\n",
    "$\\widehat{\\beta} = \\arg\\min_{\\beta \\in \\mathbb{R}^p} ||y - X \\beta||_2^2 + \\lambda_1 ||\\beta||_1 + \\lambda_2 \\beta^\\top L \\beta$\n",
    "\n",
    "where $L$ is the normalized graph Laplacian of a given network and the $\\beta^\\top L \\beta$ regularizers imposes that the entries of $\\beta$ vary smoothly on the network.\n",
    "\n",
    "Given a matrix $S \\in \\mathbb{R}^{p \\times m}$ such that $L = SS^\\top$, the ncLasso problem can be reformulated as a lasso problem of regularization parameter $\\gamma = \\frac{\\lambda_1}{\\sqrt{1+\\lambda_2}}$ by creating a \"fake\" data matrix $X_{new} \\in \\mathbb{R}^{(n+m) \\times p}$ and a corresponding response vector $y_{new} \\in \\mathbb{R}^{(n+m)}$ where\n",
    "\n",
    "$X_{new} = (1 + \\lambda_2)^{-1/2} \\begin{pmatrix} X \\\\ \\sqrt{\\lambda_2}S^\\top \\end{pmatrix}$\n",
    "\n",
    "and \n",
    "\n",
    "$y_{new} = \\begin{pmatrix} y \\\\ 0 \\end{pmatrix}$\n",
    "\n",
    "Indeed, if we call $\\beta^*$ the solution of\n",
    "\n",
    "$\\beta^* = \\arg\\min_{\\beta \\in \\mathbb{R}^p} ||y_{new} - X_{new} \\beta||_2^2 + \\gamma ||\\beta||_1,$\n",
    "\n",
    "then $\\widehat{\\beta} = \\frac{1}{\\sqrt{1+\\lambda_2}}\\beta^*.$\n",
    "\n",
    "The proof is available in Lemma 1 of the [Li and Li (2008) paper](https://academic.oup.com/bioinformatics/article/24/9/1175/206444)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd6290b-e0b5-41a5-8ed1-aca7e28cce22",
   "metadata": {},
   "source": [
    "### Defining the ncLasso class\n",
    "\n",
    "Assuming the matrix S is available, let us create a class for the ncLasso, which is not implemented in `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e02b78-292c-4478-8e69-a571a8c89f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import base, linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd9de19-9977-4930-b2e6-ac2be211449a",
   "metadata": {},
   "source": [
    "__Q: Using the equations above, fill in the code below to implement the ncLasso class__\n",
    "\n",
    "Hint: use `np.vstack` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.vstack.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f560f415-1e3a-463f-81c0-4604e1fe26d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ncLasso(base.BaseEstimator, base.RegressorMixin):\n",
    "    def __init__(self, S_transposed=None, lambda1=1.0, lambda2=1.0):\n",
    "        self.S_transposed = S_transposed # sparse matrix S_transposed\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        \n",
    "    def fit(self, X, y, max_iter=1000): \n",
    "        denominator = np.sqrt(1. + self.lambda2)\n",
    "        \n",
    "        # transformed regularization parameter\n",
    "        gamma = # TODO\n",
    "        \n",
    "        # initialize lasso\n",
    "        self.lasso = linear_model.Lasso(fit_intercept=True, \n",
    "                                        alpha=gamma, max_iter=max_iter)\n",
    "        \n",
    "        # transformed response vector\n",
    "        y_new = # TODO \n",
    "        \n",
    "        # transformed design matrix\n",
    "        X_new = # TODO \n",
    "        \n",
    "        # run the Lasso\n",
    "        self.lasso.fit(X_new, y_new)\n",
    "        \n",
    "        # transform beta (regression weights) back\n",
    "        self.coef_ = self.lasso.coef_[:X.shape[1]] / denominator\n",
    "        return self\n",
    "        \n",
    "        \n",
    "    def predict(self, X, y = None):\n",
    "        return self.lasso.predict(X)\n",
    "    \n",
    "    \n",
    "    def score(self, X, y = None):\n",
    "        return self.lasso.score(X, y)                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a271c868-6bc7-4564-8dfb-f935eb93240e",
   "metadata": {},
   "source": [
    "## Resources requirements\n",
    "\n",
    "There are two obvious choices for $S$ :\n",
    "1) The matrix $S = U \\Delta^{1/2}$, of dimension $p \\times p$, obtained from the spectral decomposition of $L$ as $L = U \\Delta U^\\top$.\n",
    "2) The incidence matrix, a $p \\times m$ matrix, where $m$ is the number of edges, and $S_{ik} \\neq 0$ iff nodes $i$ is one of the two nodes connected by edge $k$ (see for example [this blog post by Matthew N. Bernstein](https://mbernste.github.io/posts/laplacian_matrix/)).\n",
    "\n",
    "The first choice requires computing the spectral decomposition of $L$ (time consuming) and the resulting matrix $S$ is dense (memory consuming).\n",
    "The second choice requires computing the incidence matrix, which is sparse (hence memory requirements aren't intensive), and quite efficient to compute. \n",
    "However, in the first case, the matrix $X_{new}$ is of dimension $(n+p) \\times p$, which is much smaller than the $X_{new}$ from the second choice, where the dimension is $(n+m) \\times p$, and the Lasso will hence be faster to solve.\n",
    "\n",
    "Because both these options require a lot of resources (time, memory, both), it is not practical to run them on our _Arabidopsis thaliana_ dataset. For example, on my laptop, computing the spectral decomposition of the normalized Laplacian takes 22 minutes and the resulting matrix $S$ takes up more than 2GB of memory. One run of the Lasso on the transformed data (i.e. no cross-validation, for one value of $\\lambda_1$ and one value of $\\lambda_2$ takes 5-10 minutes. Computing the normalized incidence matrix doesn't take much time, but it takes several \n",
    "\n",
    "We will therefore work here with a smaller, simulated data set to illustrate the concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e829e4d5-7423-446a-940f-ba6c8bb1b312",
   "metadata": {},
   "source": [
    "## Data simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bc9607-eeb5-4210-8289-2b79712daac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data size\n",
    "num_features = 1000\n",
    "num_samples = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c807de9a-6a26-46a7-b39e-3dcf7fbbffca",
   "metadata": {},
   "source": [
    "### Graph over the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3822e7-b4f5-4481-ae63-6e6d1f31fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of a network module (all features in a module are connected)\n",
    "module_size = 10\n",
    "\n",
    "# adjacency matrix\n",
    "W_simu = np.zeros((num_features, num_features))\n",
    "for i in range(int(num_features/module_size)):\n",
    "    W_simu[i*module_size:(i+1)*module_size, i*module_size:(i+1)*module_size] = np.ones((module_size, module_size))\n",
    "    if not i == (num_features/module_size - 1):\n",
    "        W_simu[(i+1)*module_size-1, (i+1)*module_size] = 1\n",
    "        W_simu[(i+1)*module_size, (i+1)*module_size-1] = 1\n",
    "        \n",
    "# remove the diagonal\n",
    "W_simu = W_simu - np.eye(num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9979018c-5650-4d8c-8f0c-a2e7dedef1dd",
   "metadata": {},
   "source": [
    "### Simulated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c6fc8e-bddd-4247-8c0a-8b3b11f028e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "X_simu = np.random.binomial(1, 0.1, size=(num_samples, num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a50ea1b-6e29-4f97-843f-7c484210a0b8",
   "metadata": {},
   "source": [
    "### Simulated outcome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8146ecc-0074-498d-94c2-b42cd94c089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a small number of causal features\n",
    "num_causal_features = 10\n",
    "\n",
    "w_causl = np.random.normal(loc=0.2, scale=0.05, size=(num_causal_features))\n",
    "\n",
    "w = np.zeros((num_features, ))\n",
    "w[:num_causal_features] = w_causl\n",
    "\n",
    "y_simu = np.dot(X_simu, w) + np.random.normal(loc=0., scale=0.1, size=(num_samples, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9d7ea7-6a4c-40b3-bebb-e9f24d244441",
   "metadata": {},
   "source": [
    "## Lasso on the simulated data\n",
    "\n",
    "Let us now cross-validate a lasso on `(X_simu, y_simu)`. We'll only look at whether the features with non-zero coefficients match the causal features, so we don't need to split the data in train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40244727-b4fb-4c64-97e8-d63469ffc363",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = linear_model.Lasso(fit_intercept=True, max_iter=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5be2be7-3821-4cce-b1aa-f95e79a8969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddeb910-2a43-4f2b-9c41-fc6163000fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-5., -1., num=20)\n",
    "model_l1_simu = model_selection.GridSearchCV(lasso, param_grid = {'alpha': alphas}, \n",
    "                                        scoring = 'explained_variance')\n",
    "model_l1_simu.fit(X_simu, y_simu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a545a0a-d634-4600-b5a1-c961dbd3fe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(num_features), model_l1_simu.best_estimator_.coef_, label='non-causal features')\n",
    "\n",
    "# Plot the causal SNPs in red\n",
    "causal_indices = np.arange(num_causal_features)\n",
    "plt.scatter(causal_indices, model_l1_simu.best_estimator_.coef_[causal_indices], label='causal features')\n",
    "\n",
    "plt.xlabel(\"features\")\n",
    "plt.ylabel(\"ncLasso regression weight\")\n",
    "plt.xlim([0, num_features])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b951984a-6972-41bc-b8d4-57c4973a72b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The lasso selected %d features.\" % np.nonzero(model_l1_simu.best_estimator_.coef_)[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b82bb4-d2ac-4a7a-86ce-5eafb00830c1",
   "metadata": {},
   "source": [
    "__Q: How well does the Lasso identify the true causal features?__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240a3bcc-0a86-492c-84d5-32a8b1d3964e",
   "metadata": {},
   "source": [
    "## Spectral decomposition of the normalized Laplacian\n",
    "\n",
    "We will now compute the matrix S using the normalized Laplacian of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdbd792-09a3-4372-b299-91bafb1a9c59",
   "metadata": {},
   "source": [
    "__Computation of the normalized Laplacian:__\n",
    "$L_{uv} = \\begin{cases}1  & \\text{ if } u=v \\\\\n",
    "-\\frac{W_{uv}}{\\sqrt{d_u d_v}} & \\text { otherwise} \\end{cases}$\n",
    "\n",
    "where $d_u$ is the degree of node $u$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc5b8a9-aa32-44db-bda4-6e1f37210f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the node degrees\n",
    "degrees = np.sum(W_simu, axis=0)\n",
    "\n",
    "# compute the squared root of the node degrees\n",
    "degrees_sqrt = np.sqrt(degrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c801d9f-4696-4f4b-8eed-4ae92a56d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the normalized adjacency matrix \n",
    "W_normalized = W_simu / np.dot(degrees_sqrt.reshape((num_features, 1)), degrees_sqrt.reshape((1, num_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db39f75e-c927-450d-9a00-8d215b5c56b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_laplacian = np.diag(np.ones(num_features,)) - W_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7b0748-6bfd-4248-9d53-36fc515fe959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectral decomposition of the laplacian\n",
    "evals, evecs = np.linalg.eigh(normalized_laplacian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c17763-210c-49ca-99e9-37e67c8b02b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correcting for numerical errors: \n",
    "# eigenvalues of 0 might be computed as small negative numbers\n",
    "evals = np.maximum(0, evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064864fa-23d5-4ed0-9612-b583acdf14aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing S = U D^{1/2}\n",
    "laplacian_root = np.dot(evecs, np.diag(np.sqrt(evals)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028f58c0-9bcd-420b-a31e-1c9a8aea8b60",
   "metadata": {},
   "source": [
    "## Running ncLasso with fixed values of $\\lambda_1$, $\\lambda_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da7f91e-22b6-4b7a-b77f-57839220f631",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nclasso_simu = ncLasso(laplacian_root, 1e-4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7e002b-4c73-4a89-a9a2-6cb5d6ceea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nclasso_simu.fit(X_simu, y_simu, max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0132ab69-11ae-4503-864d-5debd6d9cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(num_features), model_nclasso_simu.coef_, label='non-causal features')\n",
    "\n",
    "# Plot the causal SNPs in red\n",
    "causal_indices = np.arange(num_causal_features)\n",
    "plt.scatter(causal_indices, model_nclasso_simu.coef_[causal_indices], label='causal features')\n",
    "\n",
    "plt.xlabel(\"features\")\n",
    "plt.ylabel(\"ncLasso regression weights\")\n",
    "plt.title(r\"ncLasso with $\\lambda_1 = 10^{-4}$ and $\\lambda_2 = 1$\")\n",
    "plt.xlim([0, num_features])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acda7e1f-5b5b-4aef-9122-7d4f67d390b8",
   "metadata": {},
   "source": [
    "__Q: How well does the ncLasso with regularization parameters $10^{-3}$, 10 identify the true causal features?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c5bd76-dbed-4a2c-9c64-83809ee24506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fa54146-0114-4da7-a6d9-d57a232bca1c",
   "metadata": {},
   "source": [
    "## Selecting hyperparmeters for ncLasso by cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da564012-c0cb-40da-953d-94beadfefbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_lasso = ncLasso(laplacian_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3f425a-b692-4918-bcea-eb695862aff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lambda1_list = np.logspace(-5., -3., num=5)\n",
    "lambda2_list = np.logspace(1., 2, num=5)\n",
    "\n",
    "model_nclasso_simu = model_selection.GridSearchCV(nc_lasso, param_grid = {'lambda1': lambda1_list, 'lambda2': lambda2_list}, \n",
    "                                        scoring = 'explained_variance')\n",
    "model_nclasso_simu.fit(X_simu, y_simu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b13822-e1f6-42fd-aa91-d72432f48389",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nclasso_simu.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91878b94-8eed-4b54-a70d-541a5d65e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(num_features), model_nclasso_simu.best_estimator_.coef_, label='non-causal features')\n",
    "\n",
    "# Plot the causal SNPs in red\n",
    "causal_indices = np.arange(num_causal_features)\n",
    "plt.scatter(causal_indices, model_nclasso_simu.best_estimator_.coef_[causal_indices], label='causal features')\n",
    "\n",
    "plt.xlabel(\"features\")\n",
    "plt.ylabel(\"ncLasso regression weights\")\n",
    "plt.title(\"cross-validated ncLasso\")\n",
    "plt.xlim([0, num_features])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f1e0dc-fb71-4006-bbd1-38a90f42937d",
   "metadata": {},
   "source": [
    "__Q: How well does the ncLasso identify the true causal features?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe46508-1045-4f60-b161-d39b4708989a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cd51b55-1ef7-487f-942c-da43e83d57ba",
   "metadata": {},
   "source": [
    "The ncLasso did not manage to select only the relevant features, but the number of non-relevant features is much lower than for the Lasso. Also, the coefficients associated with those non-causal features seem lower than for the Lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469ccbf0-8327-40aa-a5db-7dbafa7748fc",
   "metadata": {},
   "source": [
    "## Running the network-constrained lasso on the _A. thaliana_ data \n",
    "\n",
    "Here is code you can uncomment and use to run ncLasso on the _A. thaliana_ data. It is too time- and memory-consuming to run comfortably during our practical session, but you can play with it later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad565cf-e1ad-464a-ba9d-7cc508664056",
   "metadata": {},
   "source": [
    "### Loading the data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50605246-4fb8-4fa4-9bb4-54b42b9e660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4119986f-1608-419d-88c6-83084d5b748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading the SNP names\n",
    "# with open('data/athaliana_small.snps.txt') as f:\n",
    "#     snp_names = f.readline().split()\n",
    "#     f.close()\n",
    "# print(len(snp_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f74ca6-36f9-4425-9e43-2316d73bd739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading the design matrix -- this can take time!\n",
    "# X = np.loadtxt('data/athaliana_small.X.txt',  # file names\n",
    "#                dtype = 'int') # values are integers\n",
    "# p = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a2ea31-f8b2-44be-8b07-775d89b7eb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading the samples\n",
    "# samples = list(np.loadtxt('data/athaliana.samples.txt', # file names\n",
    "#                          dtype=int)) # values are integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433b0363-8080-4f2e-b522-b76a45831fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading the phenotypes\n",
    "# df_2W = pd.read_csv('data/athaliana.2W.pheno', # file name\n",
    "#                  header=None, # columns have no header\n",
    "#                  delim_whitespace=True, # columns are separated by white space\n",
    "#                  index_col=0) # read the first column as index\n",
    "\n",
    "# # Create vector of sample IDs\n",
    "# samples_with_phenotype_2W = list(df_2W.index)\n",
    "# print(len(samples_with_phenotype_2W), \"samples have a 2W phenotype\")\n",
    "\n",
    "# # Create vector of phenotypes\n",
    "# y_2W = df_2W[1].to_numpy()\n",
    "\n",
    "# # Restricting the design matrix to those samples who have a 2W phenotype\n",
    "# X_2W = X[np.array([samples.index(sample_id) \\\n",
    "#                    for sample_id in samples_with_phenotype_2W]), :]\n",
    "\n",
    "# # Delete X to free space\n",
    "# del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbdca13-2027-4bd6-a8e0-2e2e4370e4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading the list of candidate genes\n",
    "# with open('data/athaliana.candidates.txt') as f:\n",
    "#     candidate_genes = f.readline().split()\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d247c69c-ca2b-4683-82e2-5d73f3dc6239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading the SNPs-to-gene mapping\n",
    "# genes_by_snp = {} # key: SNP, value = [genes in/near which this SNP is]\n",
    "# with open('data/athaliana.snps_by_gene.txt') as f:\n",
    "#     for line in f:\n",
    "#         ls = line.split()\n",
    "#         gene_id = ls[0]\n",
    "#         for snp_id in ls[1:]:\n",
    "#             if not snp_id in genes_by_snp:\n",
    "#                 genes_by_snp[snp_id] = []\n",
    "#             genes_by_snp[snp_id].append(gene_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daff419-810f-451c-8b99-0fb0977d253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Splitting the data into a train and test set\n",
    "# from sklearn import model_selection\n",
    "\n",
    "# X_2W_tr, X_2W_te, y_2W_tr, y_2W_te = \\\n",
    "#     model_selection.train_test_split(X_2W, y_2W, test_size=0.2, \n",
    "#                                      random_state=17) # use the same random_state as in Notebook 1 to obtain the same split\n",
    "# print(X_2W_tr.shape, X_2W_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43551e12-2afe-43e3-9ed6-86659fd25397",
   "metadata": {},
   "source": [
    "### Sparse version of the ncLasso class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68693aa7-01cc-4a95-8b58-c7ae63f31504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a3a38-55b4-46b6-9d44-f0dd3998049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ncLasso(base.BaseEstimator, base.RegressorMixin):\n",
    "#     def __init__(self, S_transposed=None, lambda1=1.0, lambda2=1.0):\n",
    "#         self.S_transposed = S_transposed # sparse matrix S_transposed\n",
    "#         self.lambda1 = lambda1\n",
    "#         self.lambda2 = lambda2\n",
    "        \n",
    "#     def fit(self, X, y, max_iter=1000): \n",
    "#         denominator = np.sqrt(1. + self.lambda2)\n",
    "        \n",
    "#         # transformed regularization parameter\n",
    "#         gamma = self.lambda1 / denominator\n",
    "#         # initialize lasso\n",
    "#         self.lasso = linear_model.Lasso(fit_intercept=True, \n",
    "#                                         alpha=gamma, max_iter=max_iter)\n",
    "#         # transformed response vector\n",
    "#         y_new = np.hstack((y, np.zeros((self.S_transposed.shape[0], ))))\n",
    "        \n",
    "#         # transformed design matrix\n",
    "#         X_new = sparse.vstack((X, np.sqrt(self.lambda2) * self.S_transposed)) / denominator\n",
    "        \n",
    "#         # run the Lasso\n",
    "#         self.lasso.fit(X_new, y_new)\n",
    "        \n",
    "#         # transform beta (regression weights) back\n",
    "#         self.coef_ = self.lasso.coef_[:X.shape[1]] / denominator\n",
    "#         return self\n",
    "        \n",
    "        \n",
    "#     def predict(self, X, y = None):\n",
    "#         return self.lasso.predict(X)\n",
    "    \n",
    "    \n",
    "#     def score(self, X, y = None):\n",
    "#         return self.lasso.score(X, y)                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af425489-76d5-47ab-91f0-c8b01ff1fe0d",
   "metadata": {},
   "source": [
    "### Loading the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92549e41-c889-4648-9a61-edb77e5a90ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_saved = np.loadtxt('data/athaliana_small.W.txt', dtype='int') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a0c66f-c2fe-4646-8f4a-8d2286989baf",
   "metadata": {},
   "source": [
    "The network is saved in coordinate (or triplet) format: each column is an edge, with the first line being the index of one of a node, the second line the index of another node it is connected to, and the last line the edge weight.\n",
    "\n",
    "For example here `w_saved[:, 1000]` represents the 1000-th edge, which connects node index 27 to node index 2541 (with a weight of 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9674a521-4722-43d9-977c-54d98fc7f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_saved[:, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef92c3b7-2ff4-4aa3-a46e-eaf8555d86bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"The network contains %d edges\" % (w_saved.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ade2f-e372-47a0-86e0-d4ccf84ebb36",
   "metadata": {},
   "source": [
    "Such a representation corresponds to the `coo` representation of sparse matrices in `scipy.sparse`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6afa218-31b4-4278-9f0b-d3ddcf483d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create adjacency matrix in scipy.sparse.coo_matrix format\n",
    "# W = sparse.coo_matrix((w_saved[2, :], (w_saved[0, :], w_saved[1, :])), shape = (p, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7245e3e9-dcc3-420d-8683-570dbd7c65ec",
   "metadata": {},
   "source": [
    "### Option 1: Spectral decomposition of the normalized Laplacian\n",
    "On my computer, this takes 22 minutes; the matrix $U$ is 2,1 GB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be463d8-460c-49db-83ea-e70d38806dda",
   "metadata": {},
   "source": [
    "#### Computation of the matrix S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5eb8fb-edfc-4ac6-8045-eb8efec6408c",
   "metadata": {},
   "source": [
    "__Computation of the normalized Laplacian:__\n",
    "$L_{uv} = \\begin{cases}1  & \\text{ if } u=v \\\\\n",
    "-\\frac{W_{uv}}{\\sqrt{d_u d_v}} & \\text { otherwise} \\end{cases}$\n",
    "\n",
    "where $d_u$ is the degree of node $u$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20d9880-c91b-48c5-8d61-8117d9d05ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute the node degrees\n",
    "# degrees = np.sum(W, axis=0).A1.flatten()\n",
    "\n",
    "# # compute the squared root of the node degrees\n",
    "# degrees_sqrt = np.sqrt(degrees)\n",
    "\n",
    "# # normalize the edge weights\n",
    "# normalized_edge_weights = w_saved[2, :]/(degrees_sqrt[w_saved[0, :]]*degrees_sqrt[w_saved[1, :]])\n",
    "\n",
    "# # Create the normalized adjacency matrix in scipy.sparse.coo_matrix format\n",
    "# W_normalized = sparse.coo_matrix((normalized_edge_weights, (w_saved[0, :], w_saved[1, :])),\n",
    "#                                   shape = (p, p))\n",
    "\n",
    "# # Compute the normalized Laplacian\n",
    "# normalized_laplacian = sparse.diags(np.ones(p,)) - W_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4580ea-f461-48df-985d-3707a714a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # spectral decomposition of the laplacian\n",
    "# evals, evecs = np.linalg.eigh(normalized_laplacian.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a53210a-a96c-4d50-b1c9-52ff11a83379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # correcting for numerical errors: \n",
    "# # eigenvalues of 0 might be computed as small negative numbers\n",
    "# evals = np.maximum(0, evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cf076d-e133-457f-b602-635f40d1411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# laplacian_root = np.dot(evecs, np.diag(np.sqrt(evals)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c843e3b2-5f72-4b3f-8307-036e07fb4f46",
   "metadata": {},
   "source": [
    "#### Running ncLasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6435d357-f13b-461f-82c8-7e8ff8f40a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_nclasso = ncLasso(laplacian_root, 1e-4, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af75a24d-4a07-491e-8521-60a87c98b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# model_nclasso.fit(X_2W_tr, y_2W_tr, max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7143dec-1eaa-46ac-b04a-edb4cdd43591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize = (6, 4))\n",
    "# plt.scatter(range(p), # x = SNP position\n",
    "#             model_nclasso.coef_)  # y = regression weights\n",
    "\n",
    "# plt.xlabel(\"SNP\")\n",
    "# plt.ylabel(\"ncLasso regression weight\")\n",
    "# plt.xlim([0, p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7759ef2d-2b5b-48ea-89b6-0f24852bcd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"%d SNPs selected,\" % \\\n",
    "#     np.nonzero(model_nclasso.coef_)[0].shape)\n",
    "\n",
    "# candidate_genes_hit = set([])\n",
    "# num_snps_in_candidate_genes = 0\n",
    "# for snp_idx in np.nonzero(model_nclasso.coef_)[0]:\n",
    "#     for gene_id in genes_by_snp[snp_names[snp_idx]]:\n",
    "#         if gene_id in candidate_genes:\n",
    "#             candidate_genes_hit.add(gene_id)\n",
    "#             num_snps_in_candidate_genes += 1\n",
    "\n",
    "# print(\"%d SNPs are in %d candidate genes\" % (num_snps_in_candidate_genes, \n",
    "#                                                           len(candidate_genes_hit)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052de752-a5a0-4567-a278-59aae06b383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_ncl1_pred = model_nclasso.predict(X_2W_te)\n",
    "# print(\"%.3f\" % metrics.explained_variance_score(y_2W_te, y_ncl1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abac715-3154-4728-a0ae-c6a29a04bfbf",
   "metadata": {},
   "source": [
    "### Option 2: Using the normalized incidence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501e6ac-2631-457a-9da5-2271b6cfb558",
   "metadata": {},
   "source": [
    "#### Computation of the matrix S\n",
    "\n",
    "The incidence matrix $S$ is a $p \\times m$ matrix, where $m$ is the number of edges, and $S_{ik} \\neq 0$ iff nodes $i$ is one of the two nodes connected by edge $k$ (see for example [this blog post by Matthew N. Bernstein](https://mbernste.github.io/posts/laplacian_matrix/)). \n",
    "\n",
    "In the normalized version of the incidence matrix, if the $k$-th edge of the graph connects node $u$ to node $v$, with $u < v$, then $S_{uk} = \\frac{1}{\\sqrt{d_u}}$ and $S_{vk} = - \\frac{1}{\\sqrt{d_v}}.$\n",
    "\n",
    "To construct $X_{new}$, we will need $S^\\top$, so we directly compute the transposed incidence matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a545d5-01e8-418f-98c1-0b32976ad978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# incidence_t = sparse.lil_matrix((w_saved.shape[1], p), dtype=float)\n",
    "# for edge_idx, edge in enumerate(w_saved.T):\n",
    "#     if edge[0] < edge[1]:\n",
    "#         incidence_t[edge_idx, edge[0]] = 1/degrees_sqrt[edge[0]]\n",
    "#         incidence_t[edge_idx, edge[1]] = -1/degrees_sqrt[edge[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6139af-f3a0-4c46-8451-a143f6d7a401",
   "metadata": {},
   "source": [
    "#### Running ncLasso \n",
    "\n",
    "Use the same code as with Option 1 to run ncLasso with the incidence matrix. __Warning__ On my computer, this takes several hours!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3651f30-a002-48b0-8f13-e13d5aeb2d63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
